# -*- coding: utf-8 -*-
"""GST

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1omHvsOe3Zh4hMjC_eyC7SFFFfn0Z_IL7
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import IsolationForest
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Sequential

# Load datasets
x = pd.read_csv("/content/X_Train_Data_Input.csv")
y = pd.read_csv("/content/Y_Train_Data_Target.csv")

# Merge datasets on 'ID'
merged_dataset = pd.merge(x, y, on='ID', how='inner')

# Ensure the 'target' column exists and is not included in the list for dummification
target_column = 'target'

# Check for object-type columns and ensure 'target' is not mistakenly included
columns_to_dummify = merged_dataset.select_dtypes(include=['object']).columns.tolist()

id_column = 'ID'

# Check for object-type columns and ensure 'target' and 'ID' are not mistakenly included
columns_to_dummify = merged_dataset.select_dtypes(include=['object']).columns.tolist()

# Remove 'target' and 'ID' if they are present in the list of object-type columns
if target_column in columns_to_dummify:
    columns_to_dummify.remove(target_column)
if id_column in columns_to_dummify:
    columns_to_dummify.remove(id_column)

# Apply get_dummies to the remaining object-type columns
merged_dataset = pd.get_dummies(merged_dataset, columns=columns_to_dummify, drop_first=True)


# Label encode the target column
label_encoder = LabelEncoder()
merged_dataset[target_column] = label_encoder.fit_transform(merged_dataset[target_column])

# Split into features (X) and target (y)
X = merged_dataset.drop(["target", "ID"], axis="columns")
y = merged_dataset["target"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values in training set
X_train.fillna(X_train.mean(), inplace=True)

# Isolation Forest to remove anomalies
isolation_forest = IsolationForest(contamination=0.1, random_state=42)
y_train_anomalies = isolation_forest.fit_predict(X_train)

# Filter out anomalies
X_train_filtered = X_train[y_train_anomalies == 1]
y_train_filtered = y_train[y_train_anomalies == 1]

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train_filtered, y_train_filtered)

# Scale the features
scaler = MinMaxScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

# Handle missing values in test set
X_test.fillna(X_train.mean(), inplace=True)
X_test_scaled = scaler.transform(X_test)

# Cross-validation with Stratified KFold
num_epochs = 59
batch_size = 32
num_folds = 5
kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)
accuracy_scores = []

for train_index, val_index in kfold.split(X_resampled_scaled, y_resampled):
    X_train_fold, X_val_fold = X_resampled_scaled[train_index], X_resampled_scaled[val_index]
    y_train_fold, y_val_fold = y_resampled[train_index], y_resampled[val_index]

    # Define the model
    model = Sequential([
        Dense(16, input_dim=X_resampled_scaled.shape[1], activation='relu'),
        Dropout(0.5),
        Dense(8, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit(X_train_fold, y_train_fold, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val_fold, y_val_fold), verbose=0)

    # Evaluate the model
    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)
    accuracy_scores.append(val_accuracy)

# Average accuracy across folds
average_accuracy = np.mean(accuracy_scores)
print(f'Average Accuracy across {num_folds} folds: {average_accuracy:.4f}')

# Load test data for final prediction
input_test = pd.read_csv("/content/X_Test_Data_Input.csv")
target_test = pd.read_csv("/content/Y_Test_Data_Target.csv")

# Preprocess test data (remove 'ID' if present)
input_test_final = input_test.drop(["ID"], axis=1, errors='ignore')
target_test_final = target_test.drop(["ID"], axis=1, errors='ignore')

# Scale test data
input_test_final_scaled = scaler.transform(input_test_final)

# Predict on the final test set
y_pred_l = model.predict(input_test_final_scaled)
y_pred_l = np.round(y_pred_l)

# Print accuracy
accuracy = accuracy_score(target_test_final, y_pred_l)
print(f"Test Accuracy: {accuracy:.4f}")



